{
  "model": "4-Task-MultiSheaf-Test",
  "task_order": [
    "SST2",
    "IMDB",
    "Yelp",
    "AG_News"
  ],
  "task_accuracies": {
    "SST2": [
      0.86,
      0.87,
      0.8533333333333334,
      0.8133333333333334
    ],
    "IMDB": [
      0.98,
      0.9966666666666667,
      0.9966666666666667
    ],
    "Yelp": [
      0.7833333333333333,
      0.6966666666666667
    ],
    "AG_News": [
      0.86
    ]
  },
  "final_accuracies": {
    "SST2": 0.8133333333333334,
    "IMDB": 0.9966666666666667,
    "Yelp": 0.6966666666666667,
    "AG_News": 0.86
  },
  "per_task_forgetting": {
    "SST2": 0.05666666666666664,
    "IMDB": 0.0,
    "Yelp": 0.08666666666666667
  }
}