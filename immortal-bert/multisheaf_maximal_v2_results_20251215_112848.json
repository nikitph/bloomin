{
  "model": "MultiSheaf-Maximal-v2",
  "num_tasks": 10,
  "task_order": [
    "SST2",
    "IMDB",
    "Yelp",
    "QQP",
    "MRPC",
    "RTE",
    "QNLI",
    "MNLI",
    "CoLA",
    "AG_News"
  ],
  "task_accuracies": {
    "SST2": [
      0.8666666666666667,
      0.85,
      0.8266666666666667,
      0.5433333333333333,
      0.5666666666666667,
      0.6066666666666667,
      0.6333333333333333,
      0.5733333333333334,
      0.6066666666666667,
      0.5333333333333333
    ],
    "IMDB": [
      0.9766666666666667,
      0.99,
      1.0,
      1.0,
      1.0,
      1.0,
      1.0,
      1.0,
      0.9733333333333334
    ],
    "Yelp": [
      0.7366666666666667,
      0.49333333333333335,
      0.49333333333333335,
      0.49333333333333335,
      0.49333333333333335,
      0.49666666666666665,
      0.48333333333333334,
      0.49
    ],
    "QQP": [
      0.7533333333333333,
      0.7533333333333333,
      0.7266666666666667,
      0.69,
      0.6466666666666666,
      0.6766666666666666,
      0.44666666666666666
    ],
    "MRPC": [
      0.5733333333333334,
      0.6566666666666666,
      0.59,
      0.5933333333333334,
      0.56,
      0.63
    ],
    "RTE": [
      0.5631768953068592,
      0.5595667870036101,
      0.5631768953068592,
      0.5667870036101083,
      0.5270758122743683
    ],
    "QNLI": [
      0.51,
      0.5633333333333334,
      0.5433333333333333,
      0.5366666666666666
    ],
    "MNLI": [
      0.4,
      0.37666666666666665,
      0.3466666666666667
    ],
    "CoLA": [
      0.7033333333333334,
      0.65
    ],
    "AG_News": [
      0.8833333333333333
    ]
  },
  "final_accuracies": {
    "SST2": 0.5333333333333333,
    "IMDB": 0.9733333333333334,
    "Yelp": 0.49,
    "QQP": 0.44666666666666666,
    "MRPC": 0.63,
    "RTE": 0.5270758122743683,
    "QNLI": 0.5366666666666666,
    "MNLI": 0.3466666666666667,
    "CoLA": 0.65,
    "AG_News": 0.8833333333333333
  },
  "per_task_forgetting": {
    "SST2": 0.33333333333333337,
    "IMDB": 0.026666666666666616,
    "Yelp": 0.2466666666666667,
    "QQP": 0.30666666666666664,
    "MRPC": 0.026666666666666616,
    "RTE": 0.03971119133573997,
    "QNLI": 0.026666666666666727,
    "MNLI": 0.053333333333333344,
    "CoLA": 0.053333333333333344
  },
  "per_sheaf_fi": {
    "Sentiment": 0.20222222222222222,
    "Paraphrase": 0.16666666666666663,
    "Entailment": 0.03990373044524668,
    "Syntax": 0.053333333333333344,
    "Topic": 0.0
  },
  "sheaf_metrics": {
    "Sentiment": {
      "fi": 0.20222222222222222,
      "avg_accuracy": 0.6655555555555556,
      "tasks": [
        "SST2",
        "IMDB",
        "Yelp"
      ],
      "invariant": "polarity"
    },
    "Paraphrase": {
      "fi": 0.16666666666666663,
      "avg_accuracy": 0.5383333333333333,
      "tasks": [
        "QQP",
        "MRPC"
      ],
      "invariant": "symmetry"
    },
    "Entailment": {
      "fi": 0.03990373044524668,
      "avg_accuracy": 0.47013638186923384,
      "tasks": [
        "RTE",
        "QNLI",
        "MNLI"
      ],
      "invariant": "directionality"
    },
    "Syntax": {
      "fi": 0.053333333333333344,
      "avg_accuracy": 0.65,
      "tasks": [
        "CoLA"
      ],
      "invariant": "grammar"
    },
    "Topic": {
      "fi": 0.0,
      "avg_accuracy": 0.8833333333333333,
      "tasks": [
        "AG_News"
      ],
      "invariant": "clustering"
    }
  },
  "average_final_accuracy": 0.6017075812274368,
  "WTA": 0.3466666666666667,
  "FI_global": 0.12367161385211928,
  "PV": 0.033658877881896025
}