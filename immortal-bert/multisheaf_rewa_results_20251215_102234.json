{
  "model": "Multi-Sheaf-REWA",
  "num_tasks": 10,
  "task_order": [
    "SST2",
    "IMDB",
    "Yelp",
    "QQP",
    "MRPC",
    "RTE",
    "QNLI",
    "MNLI",
    "CoLA",
    "AG_News"
  ],
  "task_accuracies": {
    "SST2": [
      0.87,
      0.84,
      0.85,
      0.6866666666666666,
      0.7033333333333334,
      0.7133333333333334,
      0.7233333333333334,
      0.71,
      0.6966666666666667,
      0.65
    ],
    "IMDB": [
      0.9833333333333333,
      0.9933333333333333,
      1.0,
      1.0,
      1.0,
      1.0,
      1.0,
      1.0,
      0.9833333333333333
    ],
    "Yelp": [
      0.7933333333333333,
      0.5233333333333333,
      0.52,
      0.6,
      0.61,
      0.6033333333333334,
      0.5466666666666666,
      0.5366666666666666
    ],
    "QQP": [
      0.7466666666666667,
      0.74,
      0.69,
      0.6666666666666666,
      0.6566666666666666,
      0.6733333333333333,
      0.51
    ],
    "MRPC": [
      0.65,
      0.6533333333333333,
      0.6166666666666667,
      0.51,
      0.45666666666666667,
      0.57
    ],
    "RTE": [
      0.5667870036101083,
      0.5812274368231047,
      0.5740072202166066,
      0.555956678700361,
      0.5126353790613718
    ],
    "QNLI": [
      0.5533333333333333,
      0.5833333333333334,
      0.5633333333333334,
      0.5366666666666666
    ],
    "MNLI": [
      0.41333333333333333,
      0.4266666666666667,
      0.38
    ],
    "CoLA": [
      0.6833333333333333,
      0.7066666666666667
    ],
    "AG_News": [
      0.87
    ]
  },
  "final_accuracies": {
    "SST2": 0.65,
    "IMDB": 0.9833333333333333,
    "Yelp": 0.5366666666666666,
    "QQP": 0.51,
    "MRPC": 0.57,
    "RTE": 0.5126353790613718,
    "QNLI": 0.5366666666666666,
    "MNLI": 0.38,
    "CoLA": 0.7066666666666667,
    "AG_News": 0.87
  },
  "per_task_forgetting": {
    "SST2": 0.21999999999999997,
    "IMDB": 0.01666666666666672,
    "Yelp": 0.2566666666666667,
    "QQP": 0.2366666666666667,
    "MRPC": 0.08333333333333337,
    "RTE": 0.06859205776173294,
    "QNLI": 0.046666666666666745,
    "MNLI": 0.04666666666666669,
    "CoLA": -0.023333333333333317
  },
  "per_sheaf_fi": {
    "Sentiment": 0.16444444444444448,
    "Paraphrase": 0.16000000000000003,
    "Entailment": 0.05397513036502213,
    "Syntax": -0.023333333333333317,
    "Topic": 0.0
  },
  "sheaf_metrics": {
    "Sentiment": {
      "fi": 0.16444444444444448,
      "avg_accuracy": 0.7233333333333333,
      "tasks": [
        "SST2",
        "IMDB",
        "Yelp"
      ],
      "invariant": "polarity"
    },
    "Paraphrase": {
      "fi": 0.16000000000000003,
      "avg_accuracy": 0.54,
      "tasks": [
        "QQP",
        "MRPC"
      ],
      "invariant": "symmetry"
    },
    "Entailment": {
      "fi": 0.05397513036502213,
      "avg_accuracy": 0.47643401524267953,
      "tasks": [
        "RTE",
        "QNLI",
        "MNLI"
      ],
      "invariant": "directionality"
    },
    "Syntax": {
      "fi": -0.023333333333333317,
      "avg_accuracy": 0.7066666666666667,
      "tasks": [
        "CoLA"
      ],
      "invariant": "grammar"
    },
    "Topic": {
      "fi": 0.0,
      "avg_accuracy": 0.87,
      "tasks": [
        "AG_News"
      ],
      "invariant": "clustering"
    }
  },
  "average_final_accuracy": 0.6255968712394705,
  "WTA": 0.38,
  "FI_global": 0.10576948789945184,
  "PV": 0.030022502326369428
}