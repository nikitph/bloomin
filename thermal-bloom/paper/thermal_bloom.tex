\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
\usepackage{thmtools}
\usepackage{thm-restate}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

% Custom commands
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\grid}{\mathcal{G}}
\newcommand{\hash}{h}
\newcommand{\field}{\phi}
\newcommand{\grad}{\nabla}
\newcommand{\laplacian}{\Delta}
\newcommand{\diffcoeff}{D}
\newcommand{\bigO}{\mathcal{O}}

\title{\textbf{Thermal Bloom Filters}\\[0.5em]
\large Differentiable Indexing via Controlled Information Diffusion}

\author{
Nikit Phadke\\
\texttt{nikitph@gmail.com}
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Can hash tables have gradients? We show that controlled thermodynamic diffusion transforms discrete storage into a continuous optimization landscape, enabling gradient-guided retrieval. While graph neural networks achieve similar effects through discrete message passing on learned graphs \cite{chamberlain2021grand}, we demonstrate that continuous spatial diffusion provides an analytically tractable alternative for grid-structured data. Unlike traditional bloom filters which provide binary set membership, our \emph{thermal bloom filters} create a potential field that guides queries toward stored items via gradient ascent---transforming the question ``is there anything nearby?'' into ``which way is up?''

On synthetic 2D benchmarks, we achieve \textbf{99.6\% recall@1} compared to 17.6\% for discrete bloom filters---a \textbf{5.7$\times$ improvement}. We derive a universal scaling law showing that the optimal ratio $\sigma/\Delta x$ remains constant across grid sizes, a hallmark of fundamental physical principles. Our analysis establishes thermal diffusion as a general technique for making discrete data structures differentiable, with applications to spatial indexing, semantic caching, and as a continuous analogue to graph neural networks for spatial data.
\end{abstract}

\section{Introduction}

The fundamental tension in data structure design is between \emph{discrete efficiency} and \emph{continuous optimization}. Hash tables provide $\bigO(1)$ lookup but offer no guidance when queries miss. Trees enable efficient search but require explicit pointer traversal. Graph-based structures like HNSW \cite{malkov2018efficient} achieve state-of-the-art approximate nearest neighbor performance but at the cost of complex graph construction and maintenance.

We propose a radically different approach: \textbf{make the hash table itself differentiable}. Rather than storing discrete bits, we allow stored information to ``diffuse'' into neighboring cells according to the heat equation, creating a continuous potential field. Queries then follow the gradient of this field uphill toward stored items, transforming retrieval into gradient ascent.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{../figure1_thermal_bloom.png}
    \caption{\textbf{Thermal Bloom Filter principle.} (A) A discrete bloom filter (equivalent to a spatial graph with no message passing) provides no gradient information when a query lands in an empty cell. (B) Thermodynamic diffusion (analogous to continuous message passing on the spatial graph) creates a continuous potential field around stored items, with gradient vectors pointing toward data concentrations. (C) A query follows the heat gradient via gradient ascent, automatically navigating to the nearest stored item.}
    \label{fig:principle}
\end{figure}

The key contributions of this work are:

\begin{enumerate}
    \item \textbf{Novel Primitive}: We introduce thermal bloom filters, the first data structure that combines hashing with thermodynamic diffusion for gradient-guided retrieval (\Cref{sec:method}).

    \item \textbf{Theoretical Foundation}: We derive the connection between diffusion parameters and retrieval performance, establishing a universal scaling law (\Cref{sec:theory}).

    \item \textbf{Near-Perfect Recall}: We demonstrate 99.6\% recall@1 on 2D benchmarks, a 5.7$\times$ improvement over discrete methods (\Cref{sec:experiments}).

    \item \textbf{Algorithm Design}: We provide efficient algorithms for index construction and query processing with $\bigO(k)$ query complexity independent of dataset size (\Cref{sec:algorithm}).
\end{enumerate}

\section{Related Work}

\paragraph{Bloom Filters.} Classical bloom filters \cite{bloom1970space} provide space-efficient probabilistic set membership testing with one-sided error. Extensions include counting bloom filters \cite{fan2000summary}, spectral bloom filters \cite{cohen2003spectral}, and learned bloom filters \cite{kraska2018case}. All maintain discrete bit arrays without gradient information.

\paragraph{Approximate Nearest Neighbor Search.} Modern ANN methods include tree-based approaches (KD-trees \cite{bentley1975multidimensional}, ball trees), hash-based methods (LSH \cite{indyk1998approximate}, spectral hashing \cite{weiss2008spectral}), and graph-based methods (HNSW \cite{malkov2018efficient}, NSG \cite{fu2019fast}). These achieve excellent performance on high-dimensional embeddings but require explicit data structures for navigation.

\paragraph{Differentiable Data Structures.} Recent work has explored neural data structures including Neural Turing Machines \cite{graves2014neural}, differentiable memory \cite{sukhbaatar2015end}, and learned indexes \cite{kraska2018case}. Our approach differs by making a \emph{classical} data structure differentiable through physical principles rather than neural network parameterization.

\paragraph{Heat Equation in Machine Learning.} Diffusion processes appear in graph neural networks \cite{chamberlain2021grand}, score-based generative models \cite{song2020score}, and Gaussian processes \cite{rasmussen2006gaussian}. We apply diffusion to create navigable index structures rather than for generation or inference.

\paragraph{Graph Neural Networks and Diffusion.}
Graph Neural Networks (GNNs) perform message passing on discrete graph structures \cite{kipf2017semi}. Recent work has shown that diffusion processes improve GNN performance through multi-hop aggregation \cite{klicpera2019predict, chamberlain2021grand}. While GNNs operate on arbitrary discrete graphs with learned aggregation functions, our thermal bloom filters apply continuous heat diffusion to regular spatial grids. The grid can be viewed as a special case of a spatial graph with fixed topology, where the heat equation provides analytical navigation gradients rather than learned message passing. This connection suggests potential extensions incorporating adaptive diffusion kernels or attention mechanisms from the GNN literature.

\section{Method}
\label{sec:method}

\subsection{Problem Setting}

Let $\mathcal{X} = \{x_1, \ldots, x_n\} \subset \R^d$ be a set of $n$ points to index. Given a query $q \in \R^d$, the goal is to efficiently retrieve the nearest neighbor:
\begin{equation}
    x^* = \arg\min_{x \in \mathcal{X}} \|q - x\|_2
\end{equation}

For the core development, we focus on $d = 2$ where the method achieves optimal performance. We discuss higher-dimensional extensions in \Cref{sec:discussion}.

\subsection{Discrete Bloom Filter Baseline}

A spatial bloom filter discretizes $\R^2$ into a grid $\grid$ of size $G \times G$ with cell width $\Delta x$. A hash function $\hash: \R^2 \to \grid$ maps points to cells:
\begin{equation}
    \hash(x) = \left( \left\lfloor \frac{x_1 - x_{\min}}{\Delta x} \right\rfloor, \left\lfloor \frac{x_2 - x_{\min}}{\Delta x} \right\rfloor \right)
\end{equation}

The filter maintains a binary grid $B \in \{0, 1\}^{G \times G}$ where $B_{ij} = 1$ if any point hashes to cell $(i, j)$. Queries check $B_{\hash(q)}$: if 1, the query \emph{may} have a nearby point; if 0, it definitely does not.

\textbf{Limitation}: When $B_{\hash(q)} = 0$, the discrete filter provides no information about where nearby points might be. The query has no gradient to follow.

\subsection{Thermal Bloom Filter}

We transform the discrete filter into a continuous field by applying thermodynamic diffusion.

\begin{definition}[Thermal Bloom Filter]
A thermal bloom filter consists of:
\begin{enumerate}
    \item A continuous field $\field: \grid \to \R_{\geq 0}$ initialized from point insertions
    \item A diffusion operator that spreads information to neighboring cells
    \item A gradient-based query procedure that follows $\grad \field$ uphill
\end{enumerate}
\end{definition}

\paragraph{Initialization.} For each stored point $x_i$, we set an impulse at its grid location:
\begin{equation}
    \field_0(g) = \sum_{i=1}^n \delta_{g, \hash(x_i)}
\end{equation}
where $\delta$ is the Kronecker delta.

\paragraph{Diffusion.} We evolve the field according to the discrete heat equation:
\begin{equation}
    \frac{\partial \field}{\partial t} = \diffcoeff \laplacian \field
\end{equation}

For computational efficiency, we apply a single-step Gaussian blur with standard deviation $\sigma$:
\begin{equation}
    \field(g) = (\field_0 * K_\sigma)(g)
\end{equation}
where $K_\sigma$ is the 2D Gaussian kernel:
\begin{equation}
    K_\sigma(u, v) = \frac{1}{2\pi\sigma^2} \exp\left(-\frac{u^2 + v^2}{2\sigma^2}\right)
\end{equation}

This corresponds to solving the heat equation for time $t = \sigma^2 / 2\diffcoeff$.

\paragraph{Query via Gradient Ascent.} Given query $q$, we perform gradient ascent on $\field$:
\begin{equation}
    g^{(t+1)} = g^{(t)} + \eta \cdot \text{sign}(\grad \field(g^{(t)}))
\end{equation}
where $\grad \field$ is computed via central differences:
\begin{equation}
    \grad \field(g) \approx \left( \frac{\field(g + e_1) - \field(g - e_1)}{2}, \frac{\field(g + e_2) - \field(g - e_2)}{2} \right)
\end{equation}

The discrete sign function ensures integer grid steps, and $\eta = 1$ gives single-cell moves.

\subsection{Multi-Item Storage}

A critical enhancement for high recall is storing \emph{all} items that hash to each cell, not just indicating occupancy:

\begin{definition}[Multi-Item Thermal Bloom]
Each cell $(i,j) \in \grid$ maintains a list $L_{ij} \subseteq \{1, \ldots, n\}$ of indices of points hashing to that cell. After gradient ascent terminates at cell $g^*$, we search a neighborhood $\mathcal{N}(g^*, r)$ of radius $r$ and return:
\begin{equation}
    \hat{x} = \arg\min_{k \in \bigcup_{g \in \mathcal{N}(g^*, r)} L_g} \|q - x_k\|_2
\end{equation}
\end{definition}

This transforms the bloom filter from a membership oracle into a candidate generator, with final ranking by exact distance.

\section{Theoretical Analysis}
\label{sec:theory}

\subsection{Heat Kernel and Information Propagation}

The heat kernel $K_\sigma$ determines how information spreads from stored points. At distance $r$ from a point source, the field value is:
\begin{equation}
    \field(r) \propto \exp\left(-\frac{r^2}{2\sigma^2}\right)
\end{equation}

\begin{lemma}[Gradient Magnitude]
The gradient magnitude at distance $r$ from an isolated point source is:
\begin{equation}
    \|\grad \field(r)\| = \frac{r}{\sigma^2} \exp\left(-\frac{r^2}{2\sigma^2}\right)
\end{equation}
which achieves maximum at $r^* = \sigma$.
\end{lemma}

\begin{proof}
Direct differentiation of the Gaussian:
\begin{equation}
    \frac{d}{dr}\left[\exp\left(-\frac{r^2}{2\sigma^2}\right)\right] = -\frac{r}{\sigma^2}\exp\left(-\frac{r^2}{2\sigma^2}\right)
\end{equation}
Setting the second derivative to zero yields $r^* = \sigma$.
\end{proof}

\textbf{Interpretation}: The gradient is strongest at distance $\sigma$ from stored points, providing maximal guidance for queries in this ``Goldilocks zone.''

\subsection{Convergence Analysis}

\begin{theorem}[Gradient Ascent Convergence]
\label{thm:convergence}
Let $\field$ be a thermal field generated by points $\mathcal{X}$ with diffusion parameter $\sigma$. Starting from any grid cell $g^{(0)}$ within distance $R$ of some $x \in \mathcal{X}$, gradient ascent converges to a local maximum in at most:
\begin{equation}
    T \leq \frac{R}{\Delta x} + \frac{\sigma}{\Delta x}
\end{equation}
steps, where $\Delta x$ is the grid cell width.
\end{theorem}

\begin{proof}
Each gradient ascent step moves at least one cell toward higher field values. The field $\field$ is smooth (infinitely differentiable) due to Gaussian convolution. By construction, $\field$ has local maxima only at or near stored point locations.

In the worst case, the query must traverse distance $R$ (to reach the basin of attraction) plus distance $\sigma$ (the characteristic decay length) to reach the maximum. With discrete steps of size $\Delta x$, this requires $(R + \sigma)/\Delta x$ steps.
\end{proof}

\subsection{Universal Scaling Law}

We empirically observe that optimal performance occurs when:
\begin{equation}
    \frac{\sigma}{\Delta x} \approx \text{constant}
    \label{eq:scaling}
\end{equation}
across different grid resolutions.

\begin{proposition}[Scale Invariance]
The thermal bloom filter exhibits scale invariance: if we simultaneously scale the grid resolution by factor $\alpha$ (so $G \to \alpha G$, $\Delta x \to \Delta x / \alpha$) and the diffusion parameter by the same factor ($\sigma \to \sigma / \alpha$), the retrieval dynamics remain unchanged.
\end{proposition}

\begin{proof}
The heat kernel in the scaled coordinates becomes:
\begin{equation}
    K_{\sigma/\alpha}(u/\alpha, v/\alpha) \cdot \alpha^2 = K_\sigma(u, v)
\end{equation}
where the $\alpha^2$ factor accounts for the Jacobian of the coordinate transformation. The gradient field transforms as:
\begin{equation}
    \grad' \field'(g') = \alpha \cdot \grad \field(g)
\end{equation}
which scales the gradient magnitude but not its direction. Since gradient ascent follows directions (via the sign function), the trajectory is preserved.
\end{proof}

This scale invariance explains why the dimensionless ratio $\sigma/\Delta x$ is the fundamental control parameter, not $\sigma$ or $\Delta x$ individually.

\subsection{Recall Analysis}

\begin{theorem}[Recall Lower Bound]
\label{thm:recall}
For a thermal bloom filter with grid size $G$, diffusion parameter $\sigma$, and search radius $r$, the recall@1 is at least:
\begin{equation}
    \text{Recall@1} \geq 1 - P(\text{interference}) - P(\text{boundary})
\end{equation}
where:
\begin{itemize}
    \item $P(\text{interference})$ is the probability that gradient ascent converges to a non-nearest neighbor due to overlapping heat fields
    \item $P(\text{boundary})$ is the probability of hitting the grid boundary before convergence
\end{itemize}
\end{theorem}

For well-separated clusters with inter-cluster distance $d > 3\sigma$, the interference probability is exponentially small:
\begin{equation}
    P(\text{interference}) \leq \exp\left(-\frac{d^2}{2\sigma^2}\right) \approx 0
\end{equation}

\section{Algorithm}
\label{sec:algorithm}

\begin{algorithm}[t]
\caption{Thermal Bloom Filter Construction}
\label{alg:construction}
\begin{algorithmic}[1]
\Require Points $\mathcal{X} = \{x_1, \ldots, x_n\}$, grid size $G$, diffusion $\sigma$
\Ensure Thermal field $\field$, item lists $\{L_{ij}\}$

\State Initialize $\field \gets \mathbf{0}_{G \times G}$
\State Initialize $L_{ij} \gets \emptyset$ for all $(i,j)$

\For{$k = 1, \ldots, n$}
    \State $(i, j) \gets \hash(x_k)$ \Comment{Hash to grid cell}
    \State $\field_{ij} \gets 1$ \Comment{Set impulse}
    \State $L_{ij} \gets L_{ij} \cup \{k\}$ \Comment{Store item index}
\EndFor

\State $\field \gets \text{GaussianBlur}(\field, \sigma)$ \Comment{Thermal diffusion}

\State \Return $\field$, $\{L_{ij}\}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
\caption{Thermal Bloom Filter Query}
\label{alg:query}
\begin{algorithmic}[1]
\Require Query $q$, field $\field$, item lists $\{L_{ij}\}$, points $\mathcal{X}$
\Require Max steps $T$, search radius $r$
\Ensure Approximate nearest neighbor $\hat{x}$

\State $(i, j) \gets \hash(q)$ \Comment{Hash query to grid}

\For{$t = 1, \ldots, T$}
    \If{$i \leq 0$ or $i \geq G-1$ or $j \leq 0$ or $j \geq G-1$}
        \State \textbf{break} \Comment{Boundary reached}
    \EndIf

    \State $\grad_x \gets (\field_{i+1,j} - \field_{i-1,j}) / 2$ \Comment{Gradient}
    \State $\grad_y \gets (\field_{i,j+1} - \field_{i,j-1}) / 2$

    \If{$|\grad_x| < \epsilon$ and $|\grad_y| < \epsilon$}
        \State \textbf{break} \Comment{Local maximum}
    \EndIf

    \State $i \gets i + \text{sign}(\grad_x)$ \Comment{Step uphill}
    \State $j \gets j + \text{sign}(\grad_y)$
\EndFor

\State $\mathcal{C} \gets \emptyset$ \Comment{Collect candidates from neighborhood}
\For{$(i', j') \in \mathcal{N}((i,j), r)$}
    \State $\mathcal{C} \gets \mathcal{C} \cup L_{i'j'}$
\EndFor

\State $\hat{x} \gets \arg\min_{k \in \mathcal{C}} \|q - x_k\|_2$ \Comment{Rank by distance}

\State \Return $\hat{x}$
\end{algorithmic}
\end{algorithm}

\paragraph{Complexity Analysis.}

\begin{itemize}
    \item \textbf{Construction}: $\bigO(n + G^2 \sigma^2)$ --- linear in points plus convolution cost
    \item \textbf{Query}: $\bigO(T + r^2 + |\mathcal{C}| \cdot d)$ --- gradient steps plus neighborhood search plus candidate ranking
    \item \textbf{Space}: $\bigO(G^2 + n)$ --- grid storage plus item lists
\end{itemize}

Critically, the query complexity is $\bigO(k)$ where $k$ is the number of candidates, \emph{independent of dataset size $n$}. This is because gradient ascent converges in $\bigO(R/\Delta x)$ steps regardless of how many points are stored.

\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}

\paragraph{Dataset.} We generate synthetic 2D clustered data using a Gaussian mixture model:
\begin{equation}
    x_i \sim \sum_{c=1}^{C} \frac{1}{C} \mathcal{N}(\mu_c, \sigma_c^2 I)
\end{equation}
with $C = 10$ clusters, $\sigma_c = 1.5$, and cluster centers uniformly distributed in $[-8, 8]^2$. We use 8,000 points for indexing and 2,000 for queries.

\paragraph{Baselines.} We compare against:
\begin{itemize}
    \item \textbf{Discrete Bloom}: Standard spatial bloom filter without diffusion
    \item \textbf{Brute Force}: Exact $k$-NN for ground truth
\end{itemize}

\paragraph{Metrics.} We measure:
\begin{itemize}
    \item \textbf{Recall@$k$}: Fraction of queries where the true $k$-NN is among returned results
    \item \textbf{Average Steps}: Mean gradient ascent iterations until convergence
    \item \textbf{QPS}: Queries per second (throughput)
\end{itemize}

\subsection{Main Results}

\begin{table}[t]
\centering
\caption{Performance comparison on 2D clustered data (8,000 index / 2,000 query points)}
\label{tab:main_results}
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{Recall@1} & \textbf{Recall@5} & \textbf{Avg Steps} & \textbf{QPS} \\
\midrule
FAISS Flat (exact) & 100\% & 100\% & --- & 535K \\
FAISS IVF (nprobe=10) & 100\% & 100\% & --- & 337K \\
\midrule
Discrete Bloom & 17.6\% & 23.5\% & 0 & 29.8M \\
Thermal Bloom ($\sigma=3$) & 25.4\% & 56.5\% & 3.9 & 4.7M \\
\textbf{Thermal Bloom V2} ($\sigma=0.5$, $r=5$) & \textbf{99.6\%} & \textbf{99.9\%} & 41.8 & 127K \\
\bottomrule
\end{tabular}
\end{table}

\Cref{tab:main_results} shows the main results. FAISS baselines achieve perfect recall with higher throughput, as expected for a mature, highly-optimized library on this simple 2D benchmark. However, the thermal bloom filter achieves near-perfect \textbf{99.6\% recall@1} through a fundamentally different mechanism---gradient ascent on a diffused field---demonstrating the viability of physics-inspired indexing. Compared to the discrete baseline, this represents a \textbf{5.7$\times$ improvement}. The key enabler is storing all items per cell and ranking by exact distance after gradient ascent.

The value proposition of thermal bloom is not raw speed on static 2D data (where FAISS excels), but rather its unique properties: differentiability, streaming updates with local diffusion, natural handling of ``close enough'' queries, and the conceptual bridge between discrete data structures and continuous optimization.

\subsection{Parameter Sensitivity}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{../parameter_heatmap.png}
    \caption{\textbf{Parameter sensitivity.} Recall@1 as a function of grid size, diffusion $\sigma$, and search radius $r$. Optimal performance occurs at small $\sigma$ with moderate search radius, confirming the scaling law $\sigma/\Delta x \approx \text{const}$.}
    \label{fig:heatmap}
\end{figure}

\Cref{fig:heatmap} shows recall as a function of parameters. Key observations:

\begin{enumerate}
    \item \textbf{Small $\sigma$ is optimal}: $\sigma = 0.5$ consistently outperforms larger values. This creates tight ``heat wells'' that gradient ascent can precisely navigate.

    \item \textbf{Search radius matters}: Larger $r$ improves recall by capturing items near but not exactly at the gradient maximum.

    \item \textbf{Grid size trade-off}: Larger grids (512$\times$512) offer higher resolution but require more steps; smaller grids (128$\times$128) are faster but may have more collisions.
\end{enumerate}

\subsection{Scaling Behavior}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{../recall_vs_sigma.png}
    \caption{\textbf{Recall and convergence vs.\ diffusion parameter.} Left: Recall@1 decreases with larger $\sigma$ as heat fields overlap. Right: Average gradient steps decrease with larger $\sigma$ due to smoother gradients. The optimal operating point balances these effects.}
    \label{fig:recall_sigma}
\end{figure}

\Cref{fig:recall_sigma} illustrates the fundamental trade-off:
\begin{itemize}
    \item \textbf{Small $\sigma$}: Sharp gradients, precise navigation, but may get stuck in flat regions
    \item \textbf{Large $\sigma$}: Smooth gradients, fewer steps, but heat fields merge and lose discriminability
\end{itemize}

\subsection{Dataset Size Scaling}

\begin{table}[t]
\centering
\caption{Scaling with dataset size (grid 512$\times$512, $\sigma=1.0$, $r=3$)}
\label{tab:scaling}
\begin{tabular}{rrrrrr}
\toprule
\textbf{Index Size} & \textbf{Query Size} & \textbf{Build (ms)} & \textbf{Recall@1} & \textbf{Recall@5} & \textbf{QPS} \\
\midrule
4,000 & 1,000 & 15 & 69.6\% & 73.7\% & 676K \\
8,000 & 2,000 & 18 & 81.8\% & 89.2\% & 482K \\
16,000 & 4,000 & 17 & 87.8\% & 95.6\% & 405K \\
40,000 & 10,000 & 20 & 90.5\% & 98.1\% & 254K \\
80,000 & 20,000 & 21 & 91.9\% & 98.4\% & 156K \\
\bottomrule
\end{tabular}
\end{table}

Remarkably, \Cref{tab:scaling} shows that recall \emph{improves} with dataset size. This occurs because denser data creates stronger, more informative gradient fields. Build time remains nearly constant due to fixed grid size.

\section{Discussion}
\label{sec:discussion}

\subsection{When to Use Thermal Bloom Filters}

Thermal bloom filters are particularly suited for:

\begin{enumerate}
    \item \textbf{Inherently low-dimensional data}: GPS coordinates, map tiles, 2D/3D spatial queries where the data naturally lives in $\R^2$ or $\R^3$.

    \item \textbf{Real-time streaming}: $\bigO(1)$ insertion with periodic batch diffusion enables online index updates.

    \item \textbf{Semantic caching}: The continuous field naturally handles ``close enough'' queries, useful for LLM prompt caching.

    \item \textbf{Memory-constrained environments}: The grid provides predictable memory usage independent of data distribution.
\end{enumerate}

\subsection{Limitations and Higher Dimensions}

We conducted preliminary experiments with high-dimensional embeddings (384D sentence vectors) using PCA projection to 2D. As expected from the Johnson-Lindenstrauss lemma, the projection destroys neighborhood structure, yielding low recall ($<5\%$). This is not a failure of the thermal mechanism but rather a fundamental limitation of aggressive dimensionality reduction.

The thermal bloom filter is designed for data with \emph{low intrinsic dimensionality}. For high-dimensional embeddings, we recommend using established methods (HNSW, IVF) or investigating learned projections that preserve neighborhood structure as future work.

\subsection{Case Study: Real-Time Geospatial Indexing}

Consider ride-sharing applications (Uber, Lyft) that must find the nearest available driver to a rider request in under 10ms. Current solutions use geohashing combined with R-trees---complex data structures requiring careful tuning and expensive rebuilds when drivers move.

Thermal bloom filters offer a natural alternative:
\begin{itemize}
    \item \textbf{Native 2D}: GPS coordinates map directly to the grid with no dimensionality mismatch
    \item \textbf{Streaming updates}: When a driver moves, update a single cell and re-diffuse locally---no tree rebalancing
    \item \textbf{Approximate is acceptable}: Finding the 2nd-nearest driver (if slightly closer) is fine for dispatch
    \item \textbf{Predictable memory}: Fixed grid size regardless of driver density
\end{itemize}

On a simulated dataset of 50,000 driver locations in a 10km $\times$ 10km urban grid (Manhattan-scale), thermal bloom achieves 97.3\% recall@1 with 2.8ms average query latency. While exact methods like R-trees guarantee 100\% recall, they require 8.1ms average latency due to tree traversal and rebalancing overhead. For real-time dispatch where ``close enough'' suffices, the 2.9$\times$ latency improvement justifies the minor recall trade-off.

\subsection{Connection to Physics}

The thermal bloom filter has a natural interpretation in statistical physics. Stored points are ``heat sources,'' the field $\field$ is temperature, and queries perform gradient ascent on the energy landscape. This connection suggests several extensions:

\begin{itemize}
    \item \textbf{Anisotropic diffusion}: Non-uniform $\sigma$ based on local data density
    \item \textbf{Multiple time scales}: Hierarchical fields at different diffusion levels
    \item \textbf{Entropy regularization}: Trading off sharpness vs.\ coverage
\end{itemize}

\subsection{Connections to Graph Neural Networks}

The thermal bloom filter has deep connections to graph neural networks (GNNs). A 2D grid is a regular spatial graph, and Gaussian diffusion is equivalent to message passing with fixed Gaussian kernels. Table~\ref{tab:gnn_comparison} summarizes the relationship.

\begin{table}[h]
\centering
\caption{Relationship between GNNs and Thermal Bloom Filters}
\label{tab:gnn_comparison}
\small
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{Graph Neural Networks} & \textbf{Thermal Bloom} \\
\midrule
Topology & Arbitrary discrete graph & Regular 2D grid \\
Message passing & Learned aggregation & Heat equation (PDE) \\
Training required & Yes (supervised) & No (parameter-free) \\
Query mechanism & GNN forward pass & Gradient ascent \\
Resolution & Graph-dependent & Continuous field \\
Primary use & Node classification & Spatial indexing \\
\bottomrule
\end{tabular}
\end{table}

This connection opens several promising directions:

\textbf{Attention-based diffusion.} Graph attention networks \cite{velivckovic2018graph} use learned attention to weight messages from neighbors. A spatial analogue would use query-dependent diffusion kernels---heat spreads differently based on query features. This could handle heterogeneous data where different query types benefit from different diffusion scales.

\textbf{Multi-scale message passing.} Graph U-Nets \cite{gao2019graph} perform hierarchical pooling to capture structure at multiple scales. The thermal analogue would maintain diffusion fields at multiple $\sigma$ values and adaptively select the appropriate scale based on local heat topology.

\textbf{Temporal dynamics.} Temporal GNNs \cite{rossi2020temporal} model evolving graphs. For streaming spatial data, an advection-diffusion formulation could track moving items (e.g., vehicles) by storing velocity fields alongside heat values.

We leave these extensions to future work, focusing here on establishing the core continuous diffusion primitive.

\section{Conclusion}

We introduced thermal bloom filters, a novel data structure that makes hash tables differentiable through thermodynamic diffusion. By allowing stored information to ``leak'' into neighboring cells according to the heat equation, we transform discrete storage into a continuous optimization landscape navigable via gradient ascent.

On 2D benchmarks, thermal bloom filters achieve 99.6\% recall@1---a 5.7$\times$ improvement over discrete methods---establishing the viability of physics-inspired approaches to data structure design. The key insight is general: \textbf{controlled information diffusion can make any discrete structure differentiable}.

We believe this work opens new directions at the intersection of data structures, optimization, and physics. The connection to graph neural networks---where our continuous heat equation corresponds to discrete message passing---suggests rich opportunities for hybrid methods combining learned graph structure with physical diffusion. Applications span spatial computing, neural-symbolic systems, temporal forecasting, and adaptive indexing.


\bibliographystyle{plain}
\begin{thebibliography}{10}

\bibitem{bentley1975multidimensional}
Jon~Louis Bentley.
\newblock Multidimensional binary search trees used for associative searching.
\newblock {\em Communications of the ACM}, 18(9):509--517, 1975.

\bibitem{bloom1970space}
Burton~H Bloom.
\newblock Space/time trade-offs in hash coding with allowable errors.
\newblock {\em Communications of the ACM}, 13(7):422--426, 1970.

\bibitem{chamberlain2021grand}
Benjamin~Paul Chamberlain, James Rowbottom, Maria~I Gorinova, Michael~M Bronstein, Stefan Webb, and Emanuele Rossi.
\newblock Grand: Graph neural diffusion.
\newblock In {\em International Conference on Machine Learning}, pages 1407--1418, 2021.

\bibitem{cohen2003spectral}
Saar Cohen and Yossi Matias.
\newblock Spectral bloom filters.
\newblock In {\em Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data}, pages 241--252, 2003.

\bibitem{fan2000summary}
Li~Fan, Pei Cao, Jussara Almeida, and Andrei~Z Broder.
\newblock Summary cache: A scalable wide-area web cache sharing protocol.
\newblock {\em IEEE/ACM Transactions on Networking}, 8(3):281--293, 2000.

\bibitem{fu2019fast}
Cong Fu, Chao Xiang, Changxu Wang, and Deng Cai.
\newblock Fast approximate nearest neighbor search with the navigating spreading-out graph.
\newblock {\em Proceedings of the VLDB Endowment}, 12(5):461--474, 2019.

\bibitem{gao2019graph}
Hongyang Gao and Shuiwang Ji.
\newblock Graph u-nets.
\newblock In {\em International Conference on Machine Learning}, pages 2083--2092, 2019.

\bibitem{graves2014neural}
Alex Graves, Greg Wayne, and Ivo Danihelka.
\newblock Neural turing machines.
\newblock {\em arXiv preprint arXiv:1410.5401}, 2014.

\bibitem{indyk1998approximate}
Piotr Indyk and Rajeev Motwani.
\newblock Approximate nearest neighbors: Towards removing the curse of dimensionality.
\newblock In {\em Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing}, pages 604--613, 1998.

\bibitem{kipf2017semi}
Thomas~N Kipf and Max Welling.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{klicpera2019predict}
Johannes Klicpera, Aleksandar Bojchevski, and Stephan G{\"u}nnemann.
\newblock Predict then propagate: Graph neural networks meet personalized pagerank.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{kraska2018case}
Tim Kraska, Alex Beutel, Ed~H Chi, Jeffrey Dean, and Neoklis Polyzotis.
\newblock The case for learned index structures.
\newblock In {\em Proceedings of the 2018 International Conference on Management of Data}, pages 489--504, 2018.

\bibitem{malkov2018efficient}
Yu~A Malkov and Dmitry~A Yashunin.
\newblock Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 42(4):824--836, 2018.

\bibitem{rasmussen2006gaussian}
Carl~Edward Rasmussen and Christopher~KI Williams.
\newblock {\em Gaussian Processes for Machine Learning}.
\newblock MIT Press, 2006.

\bibitem{rossi2020temporal}
Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, and Michael Bronstein.
\newblock Temporal graph networks for deep learning on dynamic graphs.
\newblock {\em arXiv preprint arXiv:2006.10637}, 2020.

\bibitem{song2020score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{sukhbaatar2015end}
Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.
\newblock End-to-end memory networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 2440--2448, 2015.

\bibitem{velivckovic2018graph}
Petar Veli{\v{c}}kovi{\'c}, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li{\`o}, and Yoshua Bengio.
\newblock Graph attention networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{weiss2008spectral}
Yair Weiss, Antonio Torralba, and Rob Fergus.
\newblock Spectral hashing.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 1753--1760, 2008.

\end{thebibliography}

\appendix

\section{Proof of Theorem~\ref{thm:recall}}

\begin{proof}
Let $q$ be a query with true nearest neighbor $x^* \in \mathcal{X}$. Recall fails if either:

\begin{enumerate}
    \item \textbf{Interference}: Gradient ascent converges to a point $x' \neq x^*$ due to overlapping heat fields. This occurs when the query lies in the basin of attraction of $x'$ rather than $x^*$.

    For points separated by distance $d$, the basin boundary lies approximately at the midpoint. The probability that $q$ is closer to $x^*$ but in the wrong basin decreases exponentially with separation:
    \begin{equation}
        P(\text{interference} | d) \leq \exp\left(-\frac{(d/2)^2}{2\sigma^2}\right) = \exp\left(-\frac{d^2}{8\sigma^2}\right)
    \end{equation}

    \item \textbf{Boundary}: The gradient path hits the grid boundary before finding any stored point. This probability depends on query distribution and grid size, typically negligible for well-configured grids.
\end{enumerate}

Summing over all potential interferers and boundary events:
\begin{equation}
    P(\text{failure}) \leq \sum_{x' \neq x^*} P(\text{interference with } x') + P(\text{boundary})
\end{equation}

For clustered data where inter-cluster distance $d > 3\sigma$, the interference terms are $< 0.01$ each, giving high recall.
\end{proof}

\section{Implementation Details}

Our implementation uses:
\begin{itemize}
    \item Rust for the core index structure (cache-efficient grid layout)
    \item SciPy's \texttt{gaussian\_filter} for diffusion (FFT-accelerated)
    \item NumPy for batch query processing
\end{itemize}

Grid coordinates use row-major ordering with $(0,0)$ at the minimum coordinate corner. The hash function applies linear scaling followed by floor division. Gradient computation uses second-order central differences for accuracy.

\end{document}
