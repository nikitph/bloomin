//! Experiment: Can FAISS "Unlearn" via Negative Vectors?
//!
//! This experiment demonstrates that:
//! 1. Negative vectors in pointwise ANN indices do NOT implement deletion
//! 2. Operator-level destructive interference DOES implement deletion
//!
//! This is a constructive counterexample proving that deletion â‰  score manipulation.

use rand::prelude::*;
use rand_distr::StandardNormal;
use rayon::prelude::*;
use std::time::Instant;

const D: usize = 128;          // Embedding dimension
const N: usize = 100_000;      // Dataset size
const K: usize = 10;           // Top-k for retrieval
const SIGMA: f32 = 0.1;        // Kernel bandwidth for OBDS
const GRADIENT_STEPS: usize = 100;
const LEARNING_RATE: f32 = 0.01;

/// Normalize a vector to unit length
fn normalize(v: &mut [f32]) {
    let norm: f32 = v.iter().map(|x| x * x).sum::<f32>().sqrt();
    if norm > 1e-10 {
        v.iter_mut().for_each(|x| *x /= norm);
    }
}

/// Compute inner product between two vectors
fn dot(a: &[f32], b: &[f32]) -> f32 {
    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()
}

/// Compute squared Euclidean distance
fn squared_dist(a: &[f32], b: &[f32]) -> f32 {
    a.iter().zip(b.iter()).map(|(x, y)| (x - y).powi(2)).sum()
}

/// Generate random normalized vectors
fn generate_dataset(n: usize, d: usize, rng: &mut impl Rng) -> Vec<Vec<f32>> {
    (0..n)
        .map(|_| {
            let mut v: Vec<f32> = (0..d).map(|_| rng.sample(StandardNormal)).collect();
            normalize(&mut v);
            v
        })
        .collect()
}

// ============================================================================
// FAISS-LIKE FLAT INDEX (Inner Product)
// ============================================================================

struct FaissLikeIndex {
    vectors: Vec<Vec<f32>>,
}

impl FaissLikeIndex {
    fn new() -> Self {
        Self { vectors: Vec::new() }
    }

    fn add(&mut self, v: Vec<f32>) -> usize {
        let id = self.vectors.len();
        self.vectors.push(v);
        id
    }

    fn add_batch(&mut self, vecs: Vec<Vec<f32>>) {
        self.vectors.extend(vecs);
    }

    /// Search using inner product (higher = more similar)
    fn search(&self, query: &[f32], k: usize) -> Vec<(usize, f32)> {
        let mut scores: Vec<(usize, f32)> = self
            .vectors
            .par_iter()
            .enumerate()
            .map(|(i, v)| (i, dot(query, v)))
            .collect();

        // Sort by score descending
        scores.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
        scores.truncate(k);
        scores
    }

    fn len(&self) -> usize {
        self.vectors.len()
    }
}

// ============================================================================
// OBDS: Operator-Based Data Structure (Field Model)
// ============================================================================

struct ObdsField {
    /// Each entry is (center, weight) where weight can be positive or negative
    kernels: Vec<(Vec<f32>, f32)>,
    sigma: f32,
}

impl ObdsField {
    fn new(sigma: f32) -> Self {
        Self {
            kernels: Vec::new(),
            sigma,
        }
    }

    /// Add a kernel with given weight (+1 for insert, -1 for delete)
    fn add_kernel(&mut self, center: Vec<f32>, weight: f32) {
        self.kernels.push((center, weight));
    }

    /// Evaluate the field at point x: Ï†(x) = Î£ w_i * exp(-||x - x_i||Â² / 2ÏƒÂ²)
    fn evaluate(&self, x: &[f32]) -> f32 {
        self.kernels
            .iter()
            .map(|(center, weight)| {
                let d2 = squared_dist(x, center);
                weight * (-d2 / (2.0 * self.sigma * self.sigma)).exp()
            })
            .sum()
    }

    /// Compute gradient of field at point x
    fn gradient(&self, x: &[f32]) -> Vec<f32> {
        let mut grad = vec![0.0f32; x.len()];
        let sigma2 = self.sigma * self.sigma;

        for (center, weight) in &self.kernels {
            let d2 = squared_dist(x, center);
            let kernel_val = (-d2 / (2.0 * sigma2)).exp();

            // âˆ‡Ï† = Î£ w_i * K(x, x_i) * (x_i - x) / ÏƒÂ²
            for j in 0..x.len() {
                grad[j] += weight * kernel_val * (center[j] - x[j]) / sigma2;
            }
        }
        grad
    }

    /// Retrieve via gradient ascent: find local maximum of field
    /// Returns (final_position, final_field_value, converged)
    fn retrieve_gradient_ascent(&self, query: &[f32], steps: usize, lr: f32) -> (Vec<f32>, f32, bool) {
        let mut x = query.to_vec();
        let mut prev_val = self.evaluate(&x);

        for _ in 0..steps {
            let grad = self.gradient(&x);
            let grad_norm: f32 = grad.iter().map(|g| g * g).sum::<f32>().sqrt();

            // Update position
            for j in 0..x.len() {
                x[j] += lr * grad[j];
            }
            normalize(&mut x);

            let new_val = self.evaluate(&x);

            // Check convergence
            if grad_norm < 1e-6 || (new_val - prev_val).abs() < 1e-8 {
                return (x, new_val, true);
            }
            prev_val = new_val;
        }

        (x, prev_val, false)
    }

    /// Check if a point is an attractor (local maximum with positive field value)
    fn is_attractor(&self, point: &[f32], threshold: f32) -> bool {
        let val = self.evaluate(point);
        val > threshold
    }
}

// ============================================================================
// EXPERIMENT
// ============================================================================

fn run_faiss_experiment(dataset: &[Vec<f32>], target_id: usize) {
    println!("\n{}", "=".repeat(60));
    println!("EXPERIMENT 1: FAISS-Like Flat Index (Inner Product)");
    println!("{}", "=".repeat(60));

    let mut index = FaissLikeIndex::new();
    index.add_batch(dataset.to_vec());

    // Create query with small noise
    let mut rng = rand::thread_rng();
    let mut query = dataset[target_id].clone();
    for x in &mut query {
        *x += 0.01 * rng.sample::<f32, _>(StandardNormal);
    }
    normalize(&mut query);

    // Baseline search
    println!("\n[Phase 1] Baseline search (before negative vector):");
    let results = index.search(&query, K);
    let target_in_results = results.iter().any(|(id, _)| *id == target_id);
    let target_rank = results.iter().position(|(id, _)| *id == target_id);

    println!("  Top-{} results: {:?}", K, results.iter().map(|(id, _)| *id).collect::<Vec<_>>());
    println!("  Target ID {} in results: {}", target_id, target_in_results);
    if let Some(rank) = target_rank {
        println!("  Target rank: {} (score: {:.4})", rank + 1, results[rank].1);
    }

    // Insert negative vector (attempted deletion)
    println!("\n[Phase 2] Inserting negative vector -v_target...");
    let anti_vector: Vec<f32> = dataset[target_id].iter().map(|x| -x).collect();
    let anti_id = index.add(anti_vector);
    println!("  Anti-vector added with ID: {}", anti_id);
    println!("  Index size: {} vectors", index.len());

    // Search again
    println!("\n[Phase 3] Search AFTER inserting negative vector:");
    let results2 = index.search(&query, K);
    let target_in_results2 = results2.iter().any(|(id, _)| *id == target_id);
    let anti_in_results2 = results2.iter().any(|(id, _)| *id == anti_id);
    let target_rank2 = results2.iter().position(|(id, _)| *id == target_id);

    println!("  Top-{} results: {:?}", K, results2.iter().map(|(id, _)| *id).collect::<Vec<_>>());
    println!("  Target ID {} still retrievable: {} âŒ FAILURE", target_id, target_in_results2);
    println!("  Anti-vector {} in results: {}", anti_id, anti_in_results2);
    if let Some(rank) = target_rank2 {
        println!("  Target rank: {} (score: {:.4})", rank + 1, results2[rank].1);
    }

    // Analysis
    println!("\n[Analysis]");
    println!("  FAISS computes: argmax_i âŸ¨q, v_iâŸ©");
    println!("  The negative vector is a SEPARATE point, not a cancellation operator.");
    println!("  Result: Target vector remains fully retrievable.");
    println!("  âš ï¸  NEGATIVE VECTORS DO NOT IMPLEMENT DELETION IN FAISS");
}

fn run_obds_experiment(dataset: &[Vec<f32>], target_id: usize) {
    println!("\n{}", "=".repeat(60));
    println!("EXPERIMENT 2: OBDS Field Model (Gaussian Kernels)");
    println!("{}", "=".repeat(60));

    let mut field = ObdsField::new(SIGMA);

    // Add all vectors with positive weight
    println!("\n[Phase 1] Building field with {} kernels (w=+1 each)...", dataset.len());
    let start = Instant::now();
    for v in dataset {
        field.add_kernel(v.clone(), 1.0);
    }
    println!("  Field construction: {:?}", start.elapsed());

    // Create query with small noise
    let mut rng = rand::thread_rng();
    let mut query = dataset[target_id].clone();
    for x in &mut query {
        *x += 0.01 * rng.sample::<f32, _>(StandardNormal);
    }
    normalize(&mut query);

    // Baseline: evaluate field at target and retrieve via gradient ascent
    println!("\n[Phase 2] Baseline (before deletion):");
    let field_at_target = field.evaluate(&dataset[target_id]);
    println!("  Field value at target: {:.6}", field_at_target);

    let (converged_point, converged_val, did_converge) =
        field.retrieve_gradient_ascent(&query, GRADIENT_STEPS, LEARNING_RATE);
    let dist_to_target = squared_dist(&converged_point, &dataset[target_id]).sqrt();

    println!("  Gradient ascent from query:");
    println!("    Converged: {}", did_converge);
    println!("    Final field value: {:.6}", converged_val);
    println!("    Distance to target: {:.6}", dist_to_target);
    println!("    Target is attractor: {} âœ“", dist_to_target < 0.1);

    // Delete by adding negative kernel
    println!("\n[Phase 3] Deleting target via negative kernel (w=-1)...");
    field.add_kernel(dataset[target_id].clone(), -1.0);
    println!("  Added cancellation kernel at target position.");

    // Evaluate field after deletion
    println!("\n[Phase 4] After deletion:");
    let field_at_target_after = field.evaluate(&dataset[target_id]);
    println!("  Field value at target: {:.6} (was {:.6})", field_at_target_after, field_at_target);
    println!("  Field CANCELLED: {} âœ“", field_at_target_after.abs() < 0.1);

    // Try gradient ascent again
    let (converged_point2, converged_val2, did_converge2) =
        field.retrieve_gradient_ascent(&query, GRADIENT_STEPS, LEARNING_RATE);
    let dist_to_target2 = squared_dist(&converged_point2, &dataset[target_id]).sqrt();

    println!("\n  Gradient ascent from SAME query:");
    println!("    Converged: {}", did_converge2);
    println!("    Final field value: {:.6}", converged_val2);
    println!("    Distance to target: {:.6}", dist_to_target2);

    let target_still_attractor = dist_to_target2 < 0.1 && converged_val2 > 0.5;
    println!("    Target still an attractor: {} âœ“ SUCCESS", target_still_attractor);

    if !target_still_attractor {
        println!("\n  ğŸ¯ TARGET IS NO LONGER RETRIEVABLE!");
        println!("  The negative kernel created destructive interference.");
        println!("  This is O(1) deletion without index rebuild.");
    }

    println!("\n[Analysis]");
    println!("  OBDS computes: Ï†(x) = Î£ w_i Â· K(x, x_i)");
    println!("  Negative weights create destructive interference.");
    println!("  Result: Target point becomes a non-attractor (zero field).");
    println!("  âœ… OPERATOR-LEVEL DELETION SUCCEEDS");
}

fn run_additional_tests(dataset: &[Vec<f32>], _target_id: usize) {
    println!("\n{}", "=".repeat(60));
    println!("EXPERIMENT 3: Quantitative Verification");
    println!("{}", "=".repeat(60));

    // Test multiple targets
    let test_targets = [42, 1000, 5000, 10000, 50000];

    println!("\n[Multi-target deletion test]");
    println!("{:>10} {:>15} {:>15} {:>15}", "Target", "FAISS", "OBDS Pre", "OBDS Post");
    println!("{}", "-".repeat(60));

    for &tid in &test_targets {
        if tid >= dataset.len() { continue; }

        // FAISS test
        let mut faiss_idx = FaissLikeIndex::new();
        faiss_idx.add_batch(dataset.to_vec());
        let anti: Vec<f32> = dataset[tid].iter().map(|x| -x).collect();
        faiss_idx.add(anti);

        let results = faiss_idx.search(&dataset[tid], 5);
        let faiss_retrievable = results.iter().any(|(id, _)| *id == tid);

        // OBDS test
        let mut field = ObdsField::new(SIGMA);
        for v in dataset.iter().take(10000) {  // Use subset for speed
            field.add_kernel(v.clone(), 1.0);
        }

        let pre_val = field.evaluate(&dataset[tid]);
        field.add_kernel(dataset[tid].clone(), -1.0);
        let post_val = field.evaluate(&dataset[tid]);

        println!("{:>10} {:>15} {:>15.4} {:>15.4}",
                 tid,
                 if faiss_retrievable { "RETRIEVABLE" } else { "removed" },
                 pre_val,
                 post_val);
    }
}

fn print_comparison_table() {
    println!("\n{}", "=".repeat(60));
    println!("COMPARISON SUMMARY");
    println!("{}", "=".repeat(60));

    println!("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
    println!("â”‚ Metric                             â”‚  FAISS   â”‚   OBDS   â”‚");
    println!("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
    println!("â”‚ Target retrievable after deletion  â”‚    âŒ    â”‚    âœ…    â”‚");
    println!("â”‚ Rebuild required for deletion      â”‚    âœ…    â”‚    âŒ    â”‚");
    println!("â”‚ Deletion cost                      â”‚  O(N)    â”‚   O(1)   â”‚");
    println!("â”‚ Provable guarantee                 â”‚    âŒ    â”‚    âœ…    â”‚");
    println!("â”‚ GDPR/CCPA safe deletion           â”‚    âŒ    â”‚    âœ…    â”‚");
    println!("â”‚ Supports destructive interference  â”‚    âŒ    â”‚    âœ…    â”‚");
    println!("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");

    println!("\nğŸ“Œ KEY INSIGHT:");
    println!("   FAISS treats vectors as independent points.");
    println!("   OBDS treats vectors as operators in a continuous field.");
    println!("   This is not an optimizationâ€”it's a new abstraction.");
}

fn main() {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘     NEGATIVE VECTOR DELETION EXPERIMENT                      â•‘");
    println!("â•‘     Proving: FAISS â‰  Linear Field, Deletion â‰  Negation      â•‘");
    println!("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£");
    println!("â•‘  D = {}    N = {}    Ïƒ = {}                       â•‘", D, N, SIGMA);
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    // Generate dataset
    println!("\nGenerating {} random normalized vectors in R^{}...", N, D);
    let start = Instant::now();
    let mut rng = rand::thread_rng();
    let dataset = generate_dataset(N, D, &mut rng);
    println!("Dataset generated in {:?}", start.elapsed());

    let target_id = 42;
    println!("\nTarget vector ID: {}", target_id);

    // Run experiments
    run_faiss_experiment(&dataset, target_id);
    run_obds_experiment(&dataset, target_id);
    run_additional_tests(&dataset, target_id);
    print_comparison_table();

    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘  CONCLUSION                                                   â•‘");
    println!("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£");
    println!("â•‘  \"Inserting negated vectors into ANN indices does NOT        â•‘");
    println!("â•‘   remove retrievability of the original vector.              â•‘");
    println!("â•‘                                                               â•‘");
    println!("â•‘   Under an operator-based field model, signed insertions     â•‘");
    println!("â•‘   PROVABLY eliminate attractors, enabling O(1) unlearning.\" â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
}
